---
title:      Stable Diffusion
date:       2022-11-06
summary:    My experience with Stable Diffusion and comparison with Dall-E 2
categories: ["Machine Learning"]
tags: ["openai","A.I."]
---

I took a long time to publish this blog entry.
I was sitting with this post half written for weeks now,
Most stuff I want to write is out there already, even expressed better than what you will read further but I will publish this nonetheless.

SO what is this new(at the time of writing it was) thing in the town? 

# Stable Diffusion
Stable diffusion is a new model from OpenAI that is a successor to Dall-E.
The company that trained this model is stability.ai, spent a lot of time and resources to train this model.
And more importantly, they released the code and model weights for everyone to use.

{{<pict_fig src="old_superman_stubble_highly" caption="Prompt:Picture of an old superman created by Stable Diffusion" alt="Old Superman Stubble" year="22">}}

Images posted here are created by Stable Diffusion, on my laptop.
Each image is 512x512 pixels.
I am using [stable-diffusion-ui by cmdr2](https://github.com/cmdr2/stable-diffusion-ui) to generate these images.
Updates were frequent and features are added regularly.
It is great and supports a lot of features _and my hardware_.
Took me about 10 to 30 minutes to generate one depending on the number of inference steps.

{{< alert "circle-info" >}}
At the time of writing this article I have GTX 1650, 8GB RAM and intel i5-9300H.
{{< /alert >}}

Because of the log wait times I had my laptop running, generating images with input given from the webUI from my phone all thanks to the [tailscale](https://tailscale.com/) VPN. I was able to generate some really great images. For example, this one.

{{<pict_fig src="cyberpunk_city_tokyo_cinematic_realistic" caption="Prompt: Cyberpunk City Tokyo Cinematic Realistic" alt="Cyberpunk City Tokyo Cinematic Realistic" year="22">}}

This picture is first generated by stable diffusion and then scaled up to 4k using extended options of stable-diffusion-ui.
This just shows how great and extensible open source A.I. related projects can be.
It is capable at making great outputs but it is not perfect.
Getting desired output is a rare occurrence.

Getting the prompt right is essential most of the time and can be a pain.
Even with the right prompt, the output is not always what you want.
But with a beefy machine and a lot of time, you can get some really great outputs with iteration.

{{<pict_fig src="a_surrealist_photo_of_happiness_fae054aa" caption="Prompt: A Surrealist Photo of Happiness, my favourite out of the bunch" alt="A Surrealist Photo of Happiness" year="22">}}

You can get involved with online communities related to generative art and to know how to better frame the prompt.
As a developer, you can also contribute to the project and make it better.

But how is it compared to Dall-E?

## Comparison with Dall-E
It is really hard to compare these two models with context.
One is built with multi-million dollar budget and the other is built with a fraction of that.
But I will try to compare them as much as I can.
To be honest difference is not as staggering as I thought it would be.

### Prompt
These both generate images from text and images.
Stable diffusion provides much more fine grained control over the process of generating images.


Dall-E is more of a black box.
You give it a prompt and it generates an image.
And that is it.
No way to control or reproduce the output.
But what it does for you is pretty amazing.
It imagines what you want to see and generates it for you.
Unlike stable diffusion, it does not require you to know how to frame the prompt(not to the same extent at least).

In my tests with stable diffusion, image to image was really bad, dealing with abstract(for lack of a better word) idea in image was really bad.

{{<pict_fig src="minimal-dog-stable-diffusion" year="22" caption="Image to image from a minimal dog image" alt="this">}}

compared to Dall-E

![minimal dog](https://i.postimg.cc/CMJVFqP3/dog-red-clipart.png)

As you can see, stable diffusion is not able to generate a dog from a minimal dog image.
This is one of it's biggest shortcomings.

{{<pict_fig src="a_chocolate_house_with_cream_as_clouds_and_chimney" caption="Prompt: a chocolate house with cream as clouds and chimney made from sweet straw" alt="A Chocolate House with Cream as Clouds and Chimney" year="22">}}

compared to Dall-E

![a chocolate house with cream as clouds and chimney made from sweet straw](https://i.postimg.cc/q7XT543f/a-chocolate-house-with-cream-as-clouds-and-chimney-made-from-sweet-straw.png)

We can see that the stable diffusion produces comparable if not better results than Dall-E here.
Dall-E feels much more "creative" than stable diffusion but stable diffusion feels much closer to literal prompt.


{{<pict_fig src="small_house_at_the_end_of_a_cobbled_road" caption="Prompt: small house at the end of a cobbled road with a small yellowed weed choked courtyard with two chairs" alt="Small House at the End of a Cobbled Road" year="22">}}

compared to Dall-E

![small house at the end of a cobbled road with a small yellowed weed choked courtyard with two chairs](https://i.postimg.cc/50KtNLSq/small-house-at-the-end-of-a-cobbled-road-with-a-small-yellowed-weed-choked-courtyard-with-two-chairs.png)

we can see a clear lack in relation between the prompt and the output in stable diffusion.
It is not able to generate chairs.
Maybe this can be improved with more training or better prompt framing.

With this I'd like to conclude this article.
Stable diffusion is a great model and I am really excited to see what the future holds for it.
Especially with community involvement and contributions.
